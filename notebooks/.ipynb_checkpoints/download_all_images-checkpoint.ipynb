{
 "metadata": {
  "name": "",
  "signature": "sha256:da80042c515e110c49b6edea6788b793b034a9da5d820100a715e2d8043e74e1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Download All The Images\n",
      "\n",
      "You need a SpaceWarps installation plus pickles in order to be able to do the first step of reducing to a catalog. Sorry.\n",
      "\n",
      "I skipped stage1 tests that were still undecided. I think those go into stage 2. This saves a lot of images probably."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## TODO:\n",
      "- Incorporate skill into outlier_clusters_dbscan\n",
      "- add check_make for the different directories\n",
      "- add object detection for all objects in field as well as click locations\n",
      "- paths for catalogs etc are now messed up\n",
      "\n",
      "- put in the real lenses\n",
      "\n",
      "- remake cutouts using just sims and duds from stage1 as well as knownlens, and everything from stage2\n",
      "- next time when making the clusters make the filenames and alpha statuses too."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here you specify some paths for files!\n",
      "project_path = '/Volumes/Seagate/cs231n/'\n",
      "# pickles path\n",
      "base_collection_path = project_path + 'swap_pickles/09.02.15/'\n",
      "# catalog path\n",
      "annotated_catalog_path = project_path + 'catalog/annotated_catalog.csv'\n",
      "# repeat without annotations\n",
      "unannotated_catalog_path = project_path + 'catalog/unannotated_catalog.csv'\n",
      "# cluster catalog path\n",
      "cluster_catalog_path = project_path + 'catalog/cluster_catalog.csv'\n",
      "# images path\n",
      "images_path = project_path + 'images/fields/'\n",
      "# cutouts path\n",
      "cutouts_path = project_path + 'images/cutouts/'\n",
      "# cache dir for clustering\n",
      "cachedir = '../.cache'\n",
      "# knownlens catalog\n",
      "knownlens_path = project_path + 'knownlens/knownlens.csv'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here are some other config parameters\n",
      "categories = ['ID', 'ZooID', 'location', 'mean_probability', 'category', 'kind', 'flavor', 'state', 'status', 'truth',\n",
      "              'stage']\n",
      "# annotation_categories = ['At_X', 'At_Y', 'ItWas', 'Name', 'PD', 'PL']\n",
      "annotation_categories = ['At_X', 'At_Y', 'PD', 'PL']\n",
      "cluster_catalog_labels = ['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within',\n",
      "                          'cutoutname', 'fieldname']\n",
      "\n",
      "eps = int(96 * 0.5)\n",
      "min_samples = 2\n",
      "stamp_size = eps * 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import DBSCAN\n",
      "from sklearn.metrics import pairwise_distances\n",
      "from sklearn.externals import joblib\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from numpy import log2, ndarray\n",
      "\n",
      "# ======================================================================\n",
      "\n",
      "\n",
      "def outlier_clusters_dbscan(x, y, skill=None, memory=None,\n",
      "                            eps=25, min_samples=5,\n",
      "                            convert_outliers=True):\n",
      "    \"\"\"\n",
      "    DBSCAN parameters\n",
      "    eps : float, optional\n",
      "        The maximum distance between two samples for them to be considered\n",
      "        as in the same neighborhood.\n",
      "    min_samples : int, optional\n",
      "        The number of samples in a neighborhood for a point to be considered\n",
      "        as a core point.\n",
      "        \n",
      "    convert_outliers : binary, optional\n",
      "        DBSCAN assigns outliers a cluster label -1. This says take each -1 and give it a unique positive value\n",
      "    \"\"\"\n",
      "    \n",
      "    # TODO: incorporate skill\n",
      "    data = np.vstack((x, y)).T\n",
      "\n",
      "    if len(data) == 0:\n",
      "        # uh.\n",
      "        print('clustering: NO cluster members!')\n",
      "        cluster_centers = np.array([[-1, -1]])\n",
      "        cluster_labels = []\n",
      "        labels = []\n",
      "        n_clusters = 0\n",
      "        dist_within = np.array([])\n",
      "\n",
      "    elif len(data) == 1:\n",
      "        #print('clustering: only 1 data point!')\n",
      "        cluster_centers = data\n",
      "        cluster_labels = [0]\n",
      "        labels = np.array([0])\n",
      "        n_clusters = 1\n",
      "        dist_within = np.array([0])\n",
      "\n",
      "    else:\n",
      "\n",
      "        clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n",
      "        db = clusterer.fit(data)\n",
      "        labels = db.labels_\n",
      "        n_clusters = len(set(labels))\n",
      "        cluster_labels = list(set(labels))\n",
      "        if convert_outliers:\n",
      "            # now step through all the labels and set the -1s each to a new unique label\n",
      "            label_max = np.max((max(cluster_labels) + 1, 101))\n",
      "            for label_i, label in enumerate(labels):\n",
      "                if label == -1:\n",
      "                    labels[label_i] = label_max\n",
      "                    label_max += 1\n",
      "            cluster_labels = list(set(labels))\n",
      "        # cludgey\n",
      "        cluster_centers = np.array([np.mean(data[labels == i], axis=0)\n",
      "                                    for i in cluster_labels])\n",
      "        \n",
      "\n",
      "    # print n_clusters\n",
      "    # print labels\n",
      "\n",
      "    # cludgey\n",
      "    dist_within_final = np.array([np.max(pairwise_distances(\n",
      "            data[labels == i])) for i in cluster_labels])\n",
      "\n",
      "    return cluster_centers, cluster_labels, labels, n_clusters, dist_within_final\n",
      "\n",
      "outlier_clusters = outlier_clusters_dbscan\n",
      "\n",
      "def shannon(x):\n",
      "\n",
      "    if isinstance(x, ndarray) == False:\n",
      "\n",
      "        if x>0:\n",
      "            res = x*log2(x)\n",
      "        else:\n",
      "            res = 0.0\n",
      "    \n",
      "    else:\n",
      "        x[x == 0] = 1.0\n",
      "        res = x*log2(x)\n",
      "\n",
      "    return res\n",
      "\n",
      "def expectedInformationGain(p0, M_ll, M_nn):\n",
      "\n",
      "    p1 = 1-p0\n",
      "\n",
      "    I =   p0 * (shannon(M_ll) + shannon(1-M_ll)) \\\n",
      "        + p1 * (shannon(M_nn) + shannon(1-M_nn)) \\\n",
      "        - shannon(M_ll*p0 + (1-M_nn)*p1) \\\n",
      "        - shannon((1-M_ll)*p0 + M_nn*p1)\n",
      "\n",
      "    return I"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.ndimage import imread\n",
      "from scipy.misc import imsave\n",
      "from os import path\n",
      "from urllib import FancyURLopener\n",
      "\n",
      "# the fancy way of scraping images from the web; you gotta pretend you are a browser\n",
      "class MyOpener(FancyURLopener):\n",
      "    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'\n",
      "myopener = MyOpener()\n",
      "\n",
      "def get_online_png(url, outname):\n",
      "    fname = url.split('/')[-1]\n",
      "\n",
      "    # download file if we don't already have it\n",
      "    if not path.exists(outname):\n",
      "        F = myopener.retrieve(url, outname)\n",
      "    else:\n",
      "        # TODO: this is glitched?!\n",
      "        F = [outname]\n",
      "    I = imread(F[0]) * 1. / 255\n",
      "    return I"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## reduce pickles to catalog"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A catalog of stage1 and stage2 field objects with:\n",
      "    - ID (for later retrieval)\n",
      "    - probability from users\n",
      "I could probably reduce the following into one or two parameters\n",
      "    - category\n",
      "    - kind\n",
      "    - flavor\n",
      "    - state\n",
      "    - status\n",
      "    - truth\n",
      "When I make the cutouts I will also need:\n",
      "    - locations of clicks and associated skills"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import swap\n",
      "stages = range(1, 3)\n",
      "\n",
      "\n",
      "# TODO: incorporate knownlens info.\n",
      "catalog = []\n",
      "for stage in stages:\n",
      "    print(stage)\n",
      "    collection_path = base_collection_path + 'stage{0}'.format(stage) + '/CFHTLS_collection.pickle'\n",
      "    collection = swap.read_pickle(collection_path, 'collection')\n",
      "    for ID in collection.list():\n",
      "\n",
      "        subject = collection.member[ID]\n",
      "        catalog_i = []\n",
      "        \n",
      "        # for stage1 we shall skip the tests for now\n",
      "        if (stage == 1) * (subject.category == 'test'):\n",
      "            continue\n",
      "        \n",
      "        # flatten out x and y. also cut out empty entries\n",
      "        annotationhistory = subject.annotationhistory\n",
      "        x_unflat = annotationhistory['At_X']\n",
      "        x = np.array([xi for xj in x_unflat for xi in xj])\n",
      "\n",
      "        # cut out catalogs with no clicks\n",
      "        if len(x) < 1:\n",
      "            continue\n",
      "        # oh yeah there's that absolutely nutso entry with 50k clicks\n",
      "        if len(x) > 10000:\n",
      "            continue\n",
      "        \n",
      "        for category in categories:\n",
      "            if category == 'stage':\n",
      "                catalog_i.append(stage)\n",
      "            else:\n",
      "                catalog_i.append(subject.__dict__[category])\n",
      "        for category in annotation_categories:\n",
      "            catalog_i.append(list(annotationhistory[category]))\n",
      "            \n",
      "        catalog.append(catalog_i)\n",
      "catalog = pd.DataFrame(catalog, columns=categories + annotation_categories)\n",
      "\n",
      "# save catalog\n",
      "catalog.to_csv(annotated_catalog_path)\n",
      "\n",
      "# now repeat without annotations. Saves a lot of space!\n",
      "catalog[categories].to_csv(unannotated_catalog_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_types = ['category', 'kind', 'flavor', 'state', 'status', 'truth']\n",
      "for check_type in check_types:\n",
      "    print(check_type, np.unique(catalog[check_type]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('category', array(['test', 'training'], dtype=object))\n",
        "('kind', array(['dud', 'sim', 'test'], dtype=object))\n",
        "('flavor', array(['dud', u'lensed galaxy', u'lensed quasar', u'lensing cluster',\n",
        "       'test'], dtype=object))\n",
        "('state', array(['active', 'inactive'], dtype=object))\n",
        "('status', array(['detected', 'rejected', 'undecided'], dtype=object))\n",
        "('truth', array(['LENS', 'NOT', 'UNKNOWN'], dtype=object))\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: For evaluating the objects in catalog, like the annotation_categories, you will want to use ast.literal_eval(object)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Cluster Catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ast import literal_eval\n",
      "\n",
      "catalog = pd.read_csv(annotated_catalog_path)\n",
      "memory = joblib.Memory(cachedir=cachedir)\n",
      "# memory.clear()\n",
      "\n",
      "cluster_catalog = []\n",
      "ith = 0\n",
      "cluster_ID = 0\n",
      "# step through the objects in catalog\n",
      "for cati in range(len(catalog)):\n",
      "    \n",
      "    # get markers\n",
      "    x_unflat = literal_eval(catalog['At_X'][cati])\n",
      "    y_unflat = literal_eval(catalog['At_Y'][cati])\n",
      "    # flatten out x and y. also cut out empty entries\n",
      "    x = np.array([xi for xj in x_unflat for xi in xj])\n",
      "    y = np.array([yi for yj in y_unflat for yi in yj])\n",
      "    \n",
      "    # repeat for PD and PL = proxies for weight\n",
      "    PL_everyone = literal_eval(catalog['PL'][cati])\n",
      "    PL_unflat = []\n",
      "    PD_everyone = literal_eval(catalog['PD'][cati])\n",
      "    PD_unflat = []\n",
      "    for i, xj in enumerate(x_unflat):\n",
      "        # len(xj) of empty = 0\n",
      "        PL_unflat.append([PL_everyone[i]] * len(xj))\n",
      "        PD_unflat.append([PD_everyone[i]] * len(xj))\n",
      "\n",
      "    PL = np.array([PLi for PLj in PL_unflat for PLi in PLj])\n",
      "    PD = np.array([PDi for PDj in PD_unflat for PDi in PDj])\n",
      "    skill = expectedInformationGain(0.5, PL, PD)\n",
      "\n",
      "    # cluster\n",
      "    cluster_centers, cluster_center_labels, cluster_labels, n_clusters, dist_within = \\\n",
      "        outlier_clusters(x, y, skill, memory=memory, eps=eps, min_samples=min_samples)\n",
      "    \n",
      "    # add to catalog\n",
      "    for cluster_center_label_i, cluster_center_label in enumerate(cluster_center_labels):\n",
      "        \n",
      "        cluster_center = cluster_centers[cluster_center_label_i]\n",
      "        x, y = cluster_center\n",
      "\n",
      "        if np.int(np.ceil(x - stamp_size / 2)) >= 440:\n",
      "            print('Mislabled cluster? {0} {1} {2} {3}'.format(cati, cluster_center_label_i, x, y))\n",
      "            continue\n",
      "        elif np.int(np.ceil(y - stamp_size / 2)) >= 440:\n",
      "            print('Mislabled cluster? {0} {1} {2} {3}'.format(cati, cluster_center_label_i, x, y))\n",
      "            continue\n",
      "        elif np.int(np.floor(x + stamp_size / 2)) < 0:\n",
      "            print('Mislabled cluster? {0} {1} {2} {3}'.format(cati, cluster_center_label_i, x, y))\n",
      "            continue        \n",
      "        elif np.int(np.floor(y + stamp_size / 2)) < 0:\n",
      "            print('Mislabled cluster? {0} {1} {2} {3}'.format(cati, cluster_center_label_i, x, y))\n",
      "            continue\n",
      "        \n",
      "        if cluster_center_label == -1 or cluster_center_label > 100:\n",
      "            # outlier cluster\n",
      "            # so really every point is its own cluster...\n",
      "            D = 0\n",
      "        else:\n",
      "            D = dist_within[cluster_center_label]\n",
      "            \n",
      "        members = (cluster_labels == cluster_center_label)\n",
      "\n",
      "        N0 = np.sum(members)\n",
      "        S = np.sum(skill[members])\n",
      "            \n",
      "        # ['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within']\n",
      "        \n",
      "        fieldname = '{0}.png'.format(catalog['ZooID'][cati])\n",
      "        cutoutname = '{1}_{0}.png'.format(catalog['ZooID'][cati], cluster_ID)\n",
      "        cluster_catalog_i = [catalog[category][cati] for category in categories] + \\\n",
      "                            [cluster_center_label, cluster_ID, cluster_center[0], cluster_center[1], N0, S, D,\n",
      "                             cutoutname, fieldname]\n",
      "        cluster_catalog.append(cluster_catalog_i)\n",
      "        \n",
      "        cluster_ID += 1\n",
      "    \n",
      " \n",
      "cluster_catalog = pd.DataFrame(cluster_catalog, columns=categories + cluster_catalog_labels)\n",
      "\n",
      "\n",
      "# save catalog\n",
      "cluster_catalog.to_csv(cluster_catalog_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Cutouts From Catalog\n",
      "\n",
      "add an 'in_alpha' parameter to cluster catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (clusteri, len(cluster_catalog))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(16314, 35083)\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(outname_field)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Volumes/Seagate/cs231n/images/fields/ASW000526z.png\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cluster_catalog = pd.read_csv(cluster_catalog_path)\n",
      "#alphas = []\n",
      "for clusteri in range(16314 , len(cluster_catalog)):\n",
      "\n",
      "    x = cluster_catalog['x'][clusteri]\n",
      "    y = cluster_catalog['y'][clusteri]\n",
      "    \n",
      "    outname_field = images_path + cluster_catalog['fieldname'][clusteri]\n",
      "    im = get_online_png(cluster_catalog['location'][clusteri], outname_field)       \n",
      "        \n",
      "    min_x = np.int(np.floor(x - stamp_size / 2))\n",
      "    max_x = np.int(np.ceil(x + stamp_size / 2))\n",
      "    min_y = np.int(np.floor(y - stamp_size / 2))\n",
      "    max_y = np.int(np.ceil(y + stamp_size / 2))\n",
      "    pad_min_x = 0\n",
      "    pad_max_x = 0\n",
      "    pad_min_y = 0\n",
      "    pad_max_y = 0\n",
      "    pad_trip = False\n",
      "    if min_x < 0:\n",
      "        pad_min_x = -min_x\n",
      "        min_x = 0\n",
      "        pad_trip = True\n",
      "    if max_x >= im.shape[1]:\n",
      "        pad_max_x = max_x - im.shape[1]\n",
      "        max_x = im.shape[1]\n",
      "        pad_trip = True\n",
      "    if min_y < 0:\n",
      "        pad_min_y = -min_y\n",
      "        min_y = 0\n",
      "        pad_trip = True\n",
      "    if max_y >= im.shape[0]:\n",
      "        pad_max_y = max_y - im.shape[0]\n",
      "        max_y = im.shape[0]\n",
      "        pad_trip = True\n",
      "    \n",
      "    im_cut = np.pad(im[min_y: max_y, min_x: max_x],\n",
      "                    ((pad_min_y, pad_max_y), (pad_min_x, pad_max_x), (0, 0)),\n",
      "                    mode='constant')\n",
      "    im_cut = im_cut[:stamp_size, :stamp_size, :3]\n",
      "    \n",
      "    # now check if the center is in a high alpha region (if alpha is included, as it is with training)\n",
      "    alpha = 0\n",
      "    if im.shape[2] == 4:\n",
      "        im_mask = im[:,:,3].copy()\n",
      "                \n",
      "        im_mask -= im_mask.mean()\n",
      "        im_mask /= im_mask.std()\n",
      "        im_mask[im_mask < 5] = 0\n",
      "        im_mask[im_mask >= 5] = 1\n",
      "        \n",
      "        if ((y >= 0) * (y < 440) *\n",
      "            (x >= 0) * (x < 440)):\n",
      "            if im_mask[y, x] == 1:\n",
      "                alpha = 1\n",
      "    alphas.append(alpha)\n",
      "    \n",
      "    outname_cluster = cutouts_path + cluster_catalog['cutoutname'][clusteri]\n",
      "    \n",
      "    imsave(outname_cluster, im_cut)\n",
      "    \n",
      "cluster_catalog['alpha'] = alphas\n",
      "# save catalog\n",
      "cluster_catalog.to_csv(cluster_catalog_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since this took so long and I'm at 17 gb with something like 150k more clusters to go, let's stop and assess our distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now let's print some essential stats:\n",
      "cc = cluster_catalog\n",
      "\n",
      "print(cc.columns)\n",
      "\n",
      "# get number of objects in stage 1 vs stage 2\n",
      "print(cc.groupby(['stage']).apply(len))\n",
      "\n",
      "print(cc.groupby(['kind']).apply(len))\n",
      "\n",
      "print(cc.groupby(['stage', 'kind']).apply(len))\n",
      "\n",
      "# get number of flavors\n",
      "print(cc.groupby(['flavor']).apply(len))\n",
      "\n",
      "\n",
      "# combine together\n",
      "print(cc.groupby(['stage', 'flavor']).apply(len))\n",
      "print(cc.groupby(['stage', 'flavor', 'alpha']).apply(len))\n",
      "print(cc.groupby(['stage', 'flavor', 'status']).apply(len))\n",
      "\n",
      "# also show median probability\n",
      "print(cc.groupby(['stage', 'flavor'])['mean_probability'].apply(np.mean))\n",
      "print(cc.groupby(['stage', 'flavor', 'status'])['mean_probability'].apply(np.mean))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'Unnamed: 0', u'ID', u'ZooID', u'location', u'mean_probability', u'category', u'kind', u'flavor', u'state', u'status', u'truth', u'stage', u'cluster_label', u'cluster_ID', u'x', u'y', u'num_markers', u'skill_sum', u'dist_within', u'cutoutname', u'fieldname', u'alpha'], dtype='object')\n",
        "stage\n",
        "1        24177\n",
        "2        10906\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kind\n",
        "dud      9491\n",
        "sim     16562\n",
        "test     9030\n",
        "dtype: int64\n",
        "stage  kind\n",
        "1      dud      8362\n",
        "       sim     15815\n",
        "2      dud      1129\n",
        "       sim       747\n",
        "       test     9030\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "flavor\n",
        "dud                9491\n",
        "lensed galaxy      5983\n",
        "lensed quasar      5271\n",
        "lensing cluster    5308\n",
        "test               9030\n",
        "dtype: int64\n",
        "stage  flavor         \n",
        "1      dud                8362\n",
        "       lensed galaxy      5286\n",
        "       lensed quasar      5271\n",
        "       lensing cluster    5258\n",
        "2      dud                1129\n",
        "       lensed galaxy       697\n",
        "       lensing cluster      50\n",
        "       test               9030\n",
        "dtype: int64\n",
        "stage  flavor           alpha\n",
        "1      dud              0        8362\n",
        "       lensed galaxy    0        3388\n",
        "                        1        1898\n",
        "       lensed quasar    0        3351\n",
        "                        1        1920\n",
        "       lensing cluster  0        3917\n",
        "                        1        1341\n",
        "2      dud              0        1129\n",
        "       lensed galaxy    0         551\n",
        "                        1         146\n",
        "       lensing cluster  0          45\n",
        "                        1           5\n",
        "       test             0        9030\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  flavor           status   \n",
        "1      dud              detected        6\n",
        "                        rejected     8124\n",
        "                        undecided     232\n",
        "       lensed galaxy    detected     4886\n",
        "                        rejected      202\n",
        "                        undecided     198\n",
        "       lensed quasar    detected     5098\n",
        "                        rejected       59\n",
        "                        undecided     114\n",
        "       lensing cluster  detected     4845\n",
        "                        rejected      240\n",
        "                        undecided     173\n",
        "2      dud              rejected      705\n",
        "                        undecided     424\n",
        "       lensed galaxy    detected      448\n",
        "                        rejected        3\n",
        "                        undecided     246\n",
        "       lensing cluster  detected       13\n",
        "                        undecided      37\n",
        "       test             detected      151\n",
        "                        rejected     2410\n",
        "                        undecided    6469\n",
        "dtype: int64\n",
        "stage  flavor         \n",
        "1      dud                0.001022\n",
        "       lensed galaxy      0.930656\n",
        "       lensed quasar      0.969760\n",
        "       lensing cluster    0.924871\n",
        "2      dud                0.020034\n",
        "       lensed galaxy      0.837553\n",
        "       lensing cluster    0.502393\n",
        "       test               0.163845\n",
        "Name: mean_probability, dtype: float64\n",
        "stage  flavor           status   \n",
        "1      dud              detected     9.995949e-01\n",
        "                        rejected     2.182681e-08\n",
        "                        undecided    1.099119e-02\n",
        "       lensed galaxy    detected     9.999509e-01\n",
        "                        rejected     2.858481e-08\n",
        "                        undecided    1.701469e-01\n",
        "       lensed quasar    detected     9.999514e-01\n",
        "                        rejected     2.562131e-08\n",
        "                        undecided    1.214938e-01\n",
        "       lensing cluster  detected     9.999110e-01\n",
        "                        rejected     2.887855e-08\n",
        "                        undecided    1.063787e-01\n",
        "2      dud              rejected     1.603194e-09\n",
        "                        undecided    5.334435e-02\n",
        "       lensed galaxy    detected     9.983332e-01\n",
        "                        rejected     5.608823e-18\n",
        "                        undecided    5.549653e-01\n",
        "       lensing cluster  detected     9.832626e-01\n",
        "                        undecided    3.334394e-01\n",
        "       test             detected     9.965689e-01\n",
        "                        rejected     3.190369e-09\n",
        "                        undecided    2.054472e-01\n",
        "Name: mean_probability, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat for the regular catalog (by field) instead of clusters\n",
      "cc_all = catalog\n",
      "cc = cc_all\n",
      "\n",
      "# now let's print some essential stats:\n",
      "\n",
      "\n",
      "print(cc.columns)\n",
      "\n",
      "# get number of objects in stage 1 vs stage 2\n",
      "print(cc.groupby(['stage']).apply(len))\n",
      "\n",
      "print(cc.groupby(['kind']).apply(len))\n",
      "\n",
      "print(cc.groupby(['stage', 'kind']).apply(len))\n",
      "\n",
      "# get number of flavors\n",
      "print(cc.groupby(['flavor']).apply(len))\n",
      "\n",
      "\n",
      "# combine together\n",
      "print(cc.groupby(['stage', 'flavor']).apply(len))\n",
      "print(cc.groupby(['stage', 'flavor', 'status']).apply(len))\n",
      "\n",
      "# also show median probability\n",
      "print(cc.groupby(['stage', 'flavor'])['mean_probability'].apply(np.mean))\n",
      "print(cc.groupby(['stage', 'flavor', 'status'])['mean_probability'].apply(np.mean))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'Unnamed: 0', u'ID', u'ZooID', u'location', u'mean_probability', u'category', u'kind', u'flavor', u'state', u'status', u'truth', u'stage', u'At_X', u'At_Y', u'PD', u'PL'], dtype='object')\n",
        "stage\n",
        "1        8681\n",
        "2        3766\n",
        "dtype: int64\n",
        "kind\n",
        "dud     3172\n",
        "sim     5862\n",
        "test    3413\n",
        "dtype: int64\n",
        "stage  kind\n",
        "1      dud     2971\n",
        "       sim     5710\n",
        "2      dud      201\n",
        "       sim      152\n",
        "       test    3413\n",
        "dtype: int64\n",
        "flavor\n",
        "dud                3172\n",
        "lensed galaxy      2023\n",
        "lensed quasar      1930\n",
        "lensing cluster    1909\n",
        "test               3413\n",
        "dtype: int64\n",
        "stage  flavor         \n",
        "1      dud                2971\n",
        "       lensed galaxy      1879\n",
        "       lensed quasar      1930\n",
        "       lensing cluster    1901\n",
        "2      dud                 201\n",
        "       lensed galaxy       144\n",
        "       lensing cluster       8\n",
        "       test               3413\n",
        "dtype: int64\n",
        "stage  flavor           status   \n",
        "1      dud              detected        4\n",
        "                        rejected     2887\n",
        "                        undecided      80\n",
        "       lensed galaxy    detected     1735\n",
        "                        rejected       65\n",
        "                        undecided      79\n",
        "       lensed quasar    detected     1871\n",
        "                        rejected       20\n",
        "                        undecided      39\n",
        "       lensing cluster  detected     1769\n",
        "                        rejected       77\n",
        "                        undecided      55\n",
        "2      dud              rejected      127\n",
        "                        undecided      74\n",
        "       lensed galaxy    detected       94\n",
        "                        rejected        1\n",
        "                        undecided      49\n",
        "       lensing cluster  detected        2\n",
        "                        undecided       6\n",
        "       test             detected       59\n",
        "                        rejected      957\n",
        "                        undecided    2397\n",
        "dtype: int64\n",
        "stage  flavor         \n",
        "1      dud                0.001632\n",
        "       lensed galaxy      0.929710\n",
        "       lensed quasar      0.972904\n",
        "       lensing cluster    0.933985\n",
        "2      dud                0.018590\n",
        "       lensed galaxy      0.842285\n",
        "       lensing cluster    0.477581\n",
        "       test               0.161975\n",
        "Name: mean_probability, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  flavor           status   \n",
        "1      dud              detected     9.993977e-01\n",
        "                        rejected     2.182251e-08\n",
        "                        undecided    1.062773e-02\n",
        "       lensed galaxy    detected     9.999307e-01\n",
        "                        rejected     2.984655e-08\n",
        "                        undecided    1.524675e-01\n",
        "       lensed quasar    detected     9.999232e-01\n",
        "                        rejected     2.730196e-08\n",
        "                        undecided    1.756091e-01\n",
        "       lensing cluster  detected     9.999054e-01\n",
        "                        rejected     2.619193e-08\n",
        "                        undecided    1.213332e-01\n",
        "2      dud              rejected     1.277257e-09\n",
        "                        undecided    5.049312e-02\n",
        "       lensed galaxy    detected     9.983128e-01\n",
        "                        rejected     5.608823e-18\n",
        "                        undecided    5.601545e-01\n",
        "       lensing cluster  detected     9.810140e-01\n",
        "                        undecided    3.097695e-01\n",
        "       test             detected     9.953070e-01\n",
        "                        rejected     3.002540e-09\n",
        "                        undecided    2.061315e-01\n",
        "Name: mean_probability, dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## So in sum:\n",
      "\n",
      "We have something like 6700 simulated lenses, 3700 known duds, 104408 'tests' (of which we can probably safely use 93000 as duds from the stage 1 rejected tests).\n",
      "\n",
      "Data augmentation for these are clearly needed!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Compare with knownlenses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%connect_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what entries in our catalog are in the knownlens ones?\n",
      "kl = pd.read_csv(knownlens_path)\n",
      "cc = pd.read_csv(cluster_catalog_path)\n",
      "\n",
      "cc.set_index('ZooID')\n",
      "for kli in range(len(kl)):\n",
      "    kx = kl['x'][kli]\n",
      "    ky = kl['y'][kli]\n",
      "    cci = cc[kl['ZooID'][kl]]\n",
      "    break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Augment the data: Take the images, apply a reasonable affine transformation\n",
      "\n",
      "One guy actually did this in the convnet for the galaxy zoo morphologies -- we don't have to necessarily store 32x of these data if we don't want (also helps combat overfitting)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}