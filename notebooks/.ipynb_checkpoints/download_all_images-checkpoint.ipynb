{
 "metadata": {
  "name": "",
  "signature": "sha256:4840ea29be4087b0d43fbb8a61fc56f67cbe15b448e3a7be77464d9c5a32875c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Download All The Images\n",
      "\n",
      "You need a SpaceWarps installation plus pickles in order to be able to do the first step of reducing to a catalog. Sorry."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## TODO:\n",
      "- Incorporate skill into outlier_clusters_dbscan\n",
      "- Write up downloading of images and creation of cutouts\n",
      "- add check_make for the different directories\n",
      "- add object detection for all objects in field as well as click locations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import DBSCAN\n",
      "from sklearn.metrics import pairwise_distances\n",
      "from sklearn.externals import joblib\n",
      "import numpy as np\n",
      "from numpy import log2, ndarray\n",
      "\n",
      "# ======================================================================\n",
      "\n",
      "\n",
      "def outlier_clusters_dbscan(x, y, skill=None, memory=None,\n",
      "                            eps=25, min_samples=5,\n",
      "                            convert_outliers=True):\n",
      "    \"\"\"\n",
      "    DBSCAN parameters\n",
      "    eps : float, optional\n",
      "        The maximum distance between two samples for them to be considered\n",
      "        as in the same neighborhood.\n",
      "    min_samples : int, optional\n",
      "        The number of samples in a neighborhood for a point to be considered\n",
      "        as a core point.\n",
      "    \"\"\"\n",
      "    \n",
      "    # TODO: incorporate skill\n",
      "    data = np.vstack((x, y)).T\n",
      "\n",
      "    if len(data) == 0:\n",
      "        # uh.\n",
      "        print('clustering: NO cluster members!')\n",
      "        cluster_centers = np.array([[-1, -1]])\n",
      "        cluster_labels = []\n",
      "        labels = []\n",
      "        n_clusters = 0\n",
      "        dist_within = np.array([])\n",
      "\n",
      "    elif len(data) == 1:\n",
      "        #print('clustering: only 1 data point!')\n",
      "        cluster_centers = data\n",
      "        cluster_labels = [0]\n",
      "        labels = np.array([0])\n",
      "        n_clusters = 1\n",
      "        dist_within = np.array([0])\n",
      "\n",
      "    else:\n",
      "\n",
      "        clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n",
      "        db = clusterer.fit(data)\n",
      "        labels = db.labels_\n",
      "        n_clusters = len(set(labels))\n",
      "        cluster_labels = list(set(labels))\n",
      "        if convert_outliers:\n",
      "            # now step through all the labels and set the -1s each to a new unique label\n",
      "            label_max = max(cluster_labels) + 1\n",
      "            for label_i, label in enumerate(labels):\n",
      "                if label == -1:\n",
      "                    labels[label_i] = label_max\n",
      "                    label_max += 1\n",
      "            cluster_labels = list(set(labels))\n",
      "        # cludgey\n",
      "        cluster_centers = np.array([np.mean(data[labels == i], axis=0)\n",
      "                                    for i in cluster_labels])\n",
      "        \n",
      "\n",
      "    # print n_clusters\n",
      "    # print labels\n",
      "\n",
      "    # cludgey\n",
      "    dist_within_final = np.array([np.max(pairwise_distances(\n",
      "            data[labels == i])) for i in cluster_labels])\n",
      "\n",
      "    return cluster_centers, cluster_labels, labels, n_clusters, dist_within_final\n",
      "\n",
      "outlier_clusters = outlier_clusters_dbscan\n",
      "\n",
      "def shannon(x):\n",
      "\n",
      "    if isinstance(x, ndarray) == False:\n",
      "\n",
      "        if x>0:\n",
      "            res = x*log2(x)\n",
      "        else:\n",
      "            res = 0.0\n",
      "    \n",
      "    else:\n",
      "        x[x == 0] = 1.0\n",
      "        res = x*log2(x)\n",
      "\n",
      "    return res\n",
      "\n",
      "def expectedInformationGain(p0, M_ll, M_nn):\n",
      "\n",
      "    p1 = 1-p0\n",
      "\n",
      "    I =   p0 * (shannon(M_ll) + shannon(1-M_ll)) \\\n",
      "        + p1 * (shannon(M_nn) + shannon(1-M_nn)) \\\n",
      "        - shannon(M_ll*p0 + (1-M_nn)*p1) \\\n",
      "        - shannon((1-M_ll)*p0 + M_nn*p1)\n",
      "\n",
      "    return I"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.ndimage import imread\n",
      "from scipy.misc import imsave\n",
      "from os import path\n",
      "from urllib import FancyURLopener\n",
      "\n",
      "# the fancy way of scraping images from the web; you gotta pretend you are a browser\n",
      "class MyOpener(FancyURLopener):\n",
      "    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'\n",
      "myopener = MyOpener()\n",
      "\n",
      "def get_online_png(url, outname):\n",
      "    fname = url.split('/')[-1]\n",
      "\n",
      "    # download file if we don't already have it\n",
      "    if not path.exists(outname):\n",
      "        F = myopener.retrieve(url, outname)\n",
      "    else:\n",
      "        # TODO: this is glitched?!\n",
      "        F = [outname]\n",
      "    I = imread(F[0]) * 1. / 255\n",
      "    return I"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 163
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## reduce pickles to catalog"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A catalog of stage1 and stage2 field objects with:\n",
      "    - ID (for later retrieval)\n",
      "    - probability from users\n",
      "I could probably reduce the following into one or two parameters\n",
      "    - category\n",
      "    - kind\n",
      "    - flavor\n",
      "    - state\n",
      "    - status\n",
      "    - truth\n",
      "When I make the cutouts I will also need:\n",
      "    - locations of clicks and associated skills"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import swap\n",
      "import pandas as pd\n",
      "stages = [2]#range(1, 3)\n",
      "\n",
      "base_collection_path = './swap_pickles/09.02.15/'\n",
      "out_path = '../catalog/'\n",
      "\n",
      "categories = ['ID', 'ZooID', 'location', 'mean_probability', 'category', 'kind', 'flavor', 'state', 'status', 'truth']\n",
      "annotation_categories = ['At_X', 'At_Y', 'ItWas', 'Name', 'PD', 'PL']\n",
      "catalog = []\n",
      "for stage in stages:\n",
      "    collection_path = base_collection_path + 'stage{0}'.format(stage) + '/CFHTLS_collection.pickle'\n",
      "    collection = swap.read_pickle(collection_path, 'collection')\n",
      "    for ID in collection.list():\n",
      "        subject = collection.member[ID]\n",
      "        catalog_i = []\n",
      "        for category in categories:\n",
      "            catalog_i.append(subject.__dict__[category])\n",
      "            \n",
      "        annotationhistory = subject.annotationhistory\n",
      "        for category in annotation_categories:\n",
      "            catalog_i.append(list(annotationhistory[category]))\n",
      "        catalog.append(catalog_i)\n",
      "catalog = pd.DataFrame(catalog, columns=categories + annotation_categories)\n",
      "\n",
      "# save catalog\n",
      "catalog.to_csv(out_path + 'catalog.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SWAP: read an old collection of 4032 subjects from ./swap_pickles/09.02.15/stage2/CFHTLS_collection.pickle\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_types = ['category', 'kind', 'flavor', 'state', 'status', 'truth']\n",
      "for check_type in check_types:\n",
      "    print(check_type, np.unique(catalog[check_type]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('category', array(['test', 'training'], dtype=object))\n",
        "('kind', array(['dud', 'sim', 'test'], dtype=object))\n",
        "('flavor', array(['dud', 'lensed galaxy', 'lensing cluster', 'test'], dtype=object))\n",
        "('state', array(['active', 'inactive'], dtype=object))\n",
        "('status', array(['detected', 'rejected', 'undecided'], dtype=object))\n",
        "('truth', array(['LENS', 'NOT', 'UNKNOWN'], dtype=object))\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: For evaluating the objects in catalog, like the annotation_categories, you will want to use ast.literal_eval(object)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Cluster Catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ast import literal_eval\n",
      "catalog = pd.read_csv(out_path + 'catalog.csv')\n",
      "memory = joblib.Memory(cachedir='../.cache')\n",
      "memory.clear()\n",
      "\n",
      "cluster_catalog_labels = ['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within']\n",
      "cluster_catalog = []\n",
      "\n",
      "cluster_ID = 0\n",
      "eps = int(96 * 0.5)\n",
      "min_samples = 5\n",
      "# step through the objects in catalog\n",
      "for cati in range(len(catalog)):\n",
      "    # get markers\n",
      "    x_unflat = literal_eval(catalog['At_X'][cati])\n",
      "    y_unflat = literal_eval(catalog['At_Y'][cati])\n",
      "    # flatten out x and y. also cut out empty entries\n",
      "    x = np.array([xi for xj in x_unflat for xi in xj])\n",
      "    y = np.array([yi for yj in y_unflat for yi in yj])\n",
      "    \n",
      "    # repeat for PD and PL = proxies for weight\n",
      "    PL_everyone = literal_eval(catalog['PL'][cati])\n",
      "    PL_unflat = []\n",
      "    PD_everyone = literal_eval(catalog['PD'][cati])\n",
      "    PD_unflat = []\n",
      "    for i, xj in enumerate(x_unflat):\n",
      "        # len(xj) of empty = 0\n",
      "        PL_unflat.append([PL_everyone[i]] * len(xj))\n",
      "        PD_unflat.append([PD_everyone[i]] * len(xj))\n",
      "\n",
      "    PL = np.array([PLi for PLj in PL_unflat for PLi in PLj])\n",
      "    PD = np.array([PDi for PDj in PD_unflat for PDi in PDj])\n",
      "    skill = expectedInformationGain(0.5, PL, PD)\n",
      "        \n",
      "    if len(x) < 1:\n",
      "        continue\n",
      "    \n",
      "    # cluster\n",
      "    cluster_centers, cluster_center_labels, cluster_labels, n_clusters, dist_within = \\\n",
      "        outlier_clusters(x, y, skill, memory=memory, eps=eps, min_samples=min_samples)\n",
      "    \n",
      "    # add to catalog\n",
      "    for cluster_center_label in cluster_center_labels:\n",
      "        \n",
      "        if cluster_center_label == -1:\n",
      "            # outlier cluster\n",
      "            # so really every point is its own cluster...\n",
      "            D = 0\n",
      "        else:\n",
      "            D = dist_within[cluster_center_label]\n",
      "            \n",
      "        cluster_center = cluster_centers[cluster_center_label]\n",
      "        members = (cluster_labels == cluster_center_label)\n",
      "\n",
      "        N0 = np.sum(members)\n",
      "        S = np.sum(skill[members])\n",
      "            \n",
      "        # ['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within']\n",
      "        \n",
      "        cluster_catalog_i = [catalog[category][cati] for category in categories] + \\\n",
      "                            [cluster_center_label, cluster_ID, cluster_center[0], cluster_center[1], N0, S, D]\n",
      "        cluster_catalog.append(cluster_catalog_i)\n",
      "        \n",
      "        cluster_ID += 1\n",
      "    \n",
      "    # clear memory\n",
      "    #memory.clear()\n",
      "    \n",
      "cluster_catalog = pd.DataFrame(cluster_catalog, columns=categories + cluster_catalog_labels)\n",
      "\n",
      "# save catalog\n",
      "cluster_catalog.to_csv(out_path + 'cluster_catalog.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:[Memory(cachedir='../.cache/joblib')]: Flushing completely the cache\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "clustering: only 1 data point!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Cutouts From Catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pylab as plt\n",
      "#['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within']\n",
      "stamp_size = eps * 2\n",
      "cluster_catalog = pd.read_csv(out_path + 'cluster_catalog.csv')\n",
      "images_path = '../images/'\n",
      "for clusteri in range(len(cluster_catalog)):\n",
      "    outname_field = images_path + 'field_{0}.png'.format(cluster_catalog['ID'][clusteri])\n",
      "    outname_cluster = images_path + 'cutout_{0}_{1}_{2}_{3}.png'.format(cluster_catalog['flavor'][clusteri],\n",
      "                                                                        cluster_catalog['cluster_ID'][clusteri],\n",
      "                                                                        cluster_catalog['cluster_label'][clusteri],\n",
      "                                                                        cluster_catalog['ID'][clusteri])\n",
      "\n",
      "    im = get_online_png(cluster_catalog['location'][clusteri], outname_field)\n",
      "    \n",
      "    x = cluster_catalog['x'][clusteri]\n",
      "    y = cluster_catalog['y'][clusteri]\n",
      "    \n",
      "    min_x = np.int(np.floor(x - stamp_size / 2))\n",
      "    max_x = np.int(np.ceil(x + stamp_size / 2))\n",
      "    min_y = np.int(np.floor(y - stamp_size / 2))\n",
      "    max_y = np.int(np.ceil(y + stamp_size / 2))\n",
      "    pad_min_x = 0\n",
      "    pad_max_x = 0\n",
      "    pad_min_y = 0\n",
      "    pad_max_y = 0\n",
      "    pad_trip = False\n",
      "    if min_x < 0:\n",
      "        pad_min_x = -min_x\n",
      "        min_x = 0\n",
      "        pad_trip = True\n",
      "    if max_x >= im.shape[1]:\n",
      "        pad_max_x = max_x - im.shape[1]\n",
      "        max_x = im.shape[1]\n",
      "        pad_trip = True\n",
      "    if min_y < 0:\n",
      "        pad_min_y = -min_y\n",
      "        min_y = 0\n",
      "        pad_trip = True\n",
      "    if max_y >= im.shape[0]:\n",
      "        pad_max_y = max_y - im.shape[0]\n",
      "        max_y = im.shape[0]\n",
      "        pad_trip = True\n",
      "    \n",
      "    im_cut = np.pad(im[min_y: max_y, min_x: max_x],\n",
      "                    ((pad_min_y, pad_max_y), (pad_min_x, pad_max_x), (0, 0)),\n",
      "                    mode='constant')\n",
      "    im_cut = im_cut[:stamp_size, :stamp_size, :3]\n",
      "    \n",
      "    imsave(outname_cluster, im_cut)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Augment the data: Take the images, apply a reasonable affine transformation\n",
      "\n",
      "One guy actually did this in the convnet for the galaxy zoo morphologies -- we don't have to necessarily store 32x of these data if we don't want (also helps combat overfitting)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}