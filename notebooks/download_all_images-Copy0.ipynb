{
 "metadata": {
  "name": "",
  "signature": "sha256:ea69a035a9c4e8b2dd949facc7d552e55bd65a033ecaa25ac133a606fea555ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Putting here in case something DISASTROUS happens with the code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Download All The Images\n",
      "\n",
      "You need a SpaceWarps installation plus pickles in order to be able to do the first step of reducing to a catalog. Sorry.\n",
      "\n",
      "I skipped stage1 tests that were still undecided. I think those go into stage 2. This saves a lot of images probably."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## TODO:\n",
      "- Incorporate skill into outlier_clusters_dbscan\n",
      "- add check_make for the different directories\n",
      "- add object detection for all objects in field as well as click locations\n",
      "- paths for catalogs etc are now messed up\n",
      "\n",
      "- put in the real lenses\n",
      "\n",
      "- remake cutouts using just sims and duds from stage1 as well as knownlens, and everything from stage2\n",
      "- next time when making the clusters make the filenames and alpha statuses too."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here you specify some paths for files!\n",
      "# location of input catalog / pickles\n",
      "\n",
      "# catalog path\n",
      "annotated_catalog_path = '../catalog/annotated_catalog.csv'\n",
      "# cluster catalog path\n",
      "cluster_catalog_path = '../catalog/cluster_catalog.csv'\n",
      "# images path\n",
      "images_path = '../images/'\n",
      "\n",
      "# cache dir for clustering\n",
      "cachedir = '../.cache'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here are some other config parameters\n",
      "categories = ['ID', 'ZooID', 'location', 'mean_probability', 'category', 'kind', 'flavor', 'state', 'status', 'truth']\n",
      "# annotation_categories = ['At_X', 'At_Y', 'ItWas', 'Name', 'PD', 'PL']\n",
      "annotation_categories = ['At_X', 'At_Y', 'PD', 'PL']\n",
      "cluster_catalog_labels = ['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within']\n",
      "\n",
      "eps = int(96 * 0.5)\n",
      "min_samples = 2\n",
      "stamp_size = eps * 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import DBSCAN\n",
      "from sklearn.metrics import pairwise_distances\n",
      "from sklearn.externals import joblib\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from numpy import log2, ndarray\n",
      "\n",
      "# ======================================================================\n",
      "\n",
      "\n",
      "def outlier_clusters_dbscan(x, y, skill=None, memory=None,\n",
      "                            eps=25, min_samples=5,\n",
      "                            convert_outliers=True):\n",
      "    \"\"\"\n",
      "    DBSCAN parameters\n",
      "    eps : float, optional\n",
      "        The maximum distance between two samples for them to be considered\n",
      "        as in the same neighborhood.\n",
      "    min_samples : int, optional\n",
      "        The number of samples in a neighborhood for a point to be considered\n",
      "        as a core point.\n",
      "        \n",
      "    convert_outliers : binary, optional\n",
      "        DBSCAN assigns outliers a cluster label -1. This says take each -1 and give it a unique positive value\n",
      "    \"\"\"\n",
      "    \n",
      "    # TODO: incorporate skill\n",
      "    data = np.vstack((x, y)).T\n",
      "\n",
      "    if len(data) == 0:\n",
      "        # uh.\n",
      "        print('clustering: NO cluster members!')\n",
      "        cluster_centers = np.array([[-1, -1]])\n",
      "        cluster_labels = []\n",
      "        labels = []\n",
      "        n_clusters = 0\n",
      "        dist_within = np.array([])\n",
      "\n",
      "    elif len(data) == 1:\n",
      "        #print('clustering: only 1 data point!')\n",
      "        cluster_centers = data\n",
      "        cluster_labels = [0]\n",
      "        labels = np.array([0])\n",
      "        n_clusters = 1\n",
      "        dist_within = np.array([0])\n",
      "\n",
      "    else:\n",
      "\n",
      "        clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n",
      "        db = clusterer.fit(data)\n",
      "        labels = db.labels_\n",
      "        n_clusters = len(set(labels))\n",
      "        cluster_labels = list(set(labels))\n",
      "        if convert_outliers:\n",
      "            # now step through all the labels and set the -1s each to a new unique label\n",
      "            label_max = np.max((max(cluster_labels) + 1, 101))\n",
      "            for label_i, label in enumerate(labels):\n",
      "                if label == -1:\n",
      "                    labels[label_i] = label_max\n",
      "                    label_max += 1\n",
      "            cluster_labels = list(set(labels))\n",
      "        # cludgey\n",
      "        cluster_centers = np.array([np.mean(data[labels == i], axis=0)\n",
      "                                    for i in cluster_labels])\n",
      "        \n",
      "\n",
      "    # print n_clusters\n",
      "    # print labels\n",
      "\n",
      "    # cludgey\n",
      "    dist_within_final = np.array([np.max(pairwise_distances(\n",
      "            data[labels == i])) for i in cluster_labels])\n",
      "\n",
      "    return cluster_centers, cluster_labels, labels, n_clusters, dist_within_final\n",
      "\n",
      "outlier_clusters = outlier_clusters_dbscan\n",
      "\n",
      "def shannon(x):\n",
      "\n",
      "    if isinstance(x, ndarray) == False:\n",
      "\n",
      "        if x>0:\n",
      "            res = x*log2(x)\n",
      "        else:\n",
      "            res = 0.0\n",
      "    \n",
      "    else:\n",
      "        x[x == 0] = 1.0\n",
      "        res = x*log2(x)\n",
      "\n",
      "    return res\n",
      "\n",
      "def expectedInformationGain(p0, M_ll, M_nn):\n",
      "\n",
      "    p1 = 1-p0\n",
      "\n",
      "    I =   p0 * (shannon(M_ll) + shannon(1-M_ll)) \\\n",
      "        + p1 * (shannon(M_nn) + shannon(1-M_nn)) \\\n",
      "        - shannon(M_ll*p0 + (1-M_nn)*p1) \\\n",
      "        - shannon((1-M_ll)*p0 + M_nn*p1)\n",
      "\n",
      "    return I"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.ndimage import imread\n",
      "from scipy.misc import imsave\n",
      "from os import path\n",
      "from urllib import FancyURLopener\n",
      "\n",
      "# the fancy way of scraping images from the web; you gotta pretend you are a browser\n",
      "class MyOpener(FancyURLopener):\n",
      "    version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'\n",
      "myopener = MyOpener()\n",
      "\n",
      "def get_online_png(url, outname):\n",
      "    fname = url.split('/')[-1]\n",
      "\n",
      "    # download file if we don't already have it\n",
      "    if not path.exists(outname):\n",
      "        F = myopener.retrieve(url, outname)\n",
      "    else:\n",
      "        # TODO: this is glitched?!\n",
      "        F = [outname]\n",
      "    I = imread(F[0]) * 1. / 255\n",
      "    return I"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## reduce pickles to catalog"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A catalog of stage1 and stage2 field objects with:\n",
      "    - ID (for later retrieval)\n",
      "    - probability from users\n",
      "I could probably reduce the following into one or two parameters\n",
      "    - category\n",
      "    - kind\n",
      "    - flavor\n",
      "    - state\n",
      "    - status\n",
      "    - truth\n",
      "When I make the cutouts I will also need:\n",
      "    - locations of clicks and associated skills"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "import swap\n",
      "stages = range(1, 3)\n",
      "\n",
      "base_collection_path = './09.02.15/'\n",
      "out_path = '../catalog/'\n",
      "\n",
      "# TODO: incorporate knownlens info.\n",
      "catalog = []\n",
      "for stage in stages:\n",
      "    collection_path = base_collection_path + 'stage{0}'.format(stage) + '/CFHTLS_collection.pickle'\n",
      "    collection = swap.read_pickle(collection_path, 'collection')\n",
      "    for ID in collection.list():\n",
      "        subject = collection.member[ID]\n",
      "        catalog_i = []\n",
      "        for category in categories:\n",
      "            catalog_i.append(subject.__dict__[category])    \n",
      "        annotationhistory = subject.annotationhistory\n",
      "        for category in annotation_categories:\n",
      "            catalog_i.append(list(annotationhistory[category]))\n",
      "        catalog.append(catalog_i)\n",
      "catalog = pd.DataFrame(catalog, columns=categories + annotation_categories)\n",
      "\n",
      "# save catalog\n",
      "catalog.to_csv(out_path + 'base_catalog_annotated.csv')\n",
      "\n",
      "# now repeat without annotations. Saves a lot of space!\n",
      "catalog[categories].to_csv(out_path + 'base_catalog_unannotated.csv')"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_types = ['category', 'kind', 'flavor', 'state', 'status', 'truth']\n",
      "for check_type in check_types:\n",
      "    print(check_type, np.unique(catalog[check_type]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('category', array(['test', 'training'], dtype=object))\n",
        "('kind', array(['dud', 'sim', 'test'], dtype=object))\n",
        "('flavor', array(['dud', 'lensed galaxy', 'lensing cluster', 'test'], dtype=object))\n",
        "('state', array(['active', 'inactive'], dtype=object))\n",
        "('status', array(['detected', 'rejected', 'undecided'], dtype=object))\n",
        "('truth', array(['LENS', 'NOT', 'UNKNOWN'], dtype=object))\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: For evaluating the objects in catalog, like the annotation_categories, you will want to use ast.literal_eval(object)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Cluster Catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ast import literal_eval\n",
      "\n",
      "catalog = pd.read_csv(annotated_catalog_path)\n",
      "memory = joblib.Memory(cachedir=cachedir)\n",
      "# memory.clear()\n",
      "\n",
      "cluster_catalog = []\n",
      "ith = 0\n",
      "cluster_ID = 0\n",
      "# step through the objects in catalog\n",
      "for cati in range(len(catalog)):\n",
      "    # for stage1 we shall skip the undecided tests for now\n",
      "    if (stage == 1) * (catalog['category'][cati] == 'test') * (catalog['status'][cati] == 'undecided'):\n",
      "        continue\n",
      "    \n",
      "    # get markers\n",
      "    x_unflat = literal_eval(catalog['At_X'][cati])\n",
      "    y_unflat = literal_eval(catalog['At_Y'][cati])\n",
      "    # flatten out x and y. also cut out empty entries\n",
      "    x = np.array([xi for xj in x_unflat for xi in xj])\n",
      "    y = np.array([yi for yj in y_unflat for yi in yj])\n",
      "    \n",
      "    # repeat for PD and PL = proxies for weight\n",
      "    PL_everyone = literal_eval(catalog['PL'][cati])\n",
      "    PL_unflat = []\n",
      "    PD_everyone = literal_eval(catalog['PD'][cati])\n",
      "    PD_unflat = []\n",
      "    for i, xj in enumerate(x_unflat):\n",
      "        # len(xj) of empty = 0\n",
      "        PL_unflat.append([PL_everyone[i]] * len(xj))\n",
      "        PD_unflat.append([PD_everyone[i]] * len(xj))\n",
      "\n",
      "    PL = np.array([PLi for PLj in PL_unflat for PLi in PLj])\n",
      "    PD = np.array([PDi for PDj in PD_unflat for PDi in PDj])\n",
      "    skill = expectedInformationGain(0.5, PL, PD)\n",
      "        \n",
      "    if len(x) < 1:\n",
      "        continue\n",
      "    # oh yeah there's that absolutely nutso entry with 50k clicks\n",
      "    if len(x) > 10000:\n",
      "        continue\n",
      "    # cluster\n",
      "    cluster_centers, cluster_center_labels, cluster_labels, n_clusters, dist_within = \\\n",
      "        outlier_clusters(x, y, skill, memory=memory, eps=eps, min_samples=min_samples)\n",
      "    \n",
      "    # add to catalog\n",
      "    for cluster_center_label_i, cluster_center_label in enumerate(cluster_center_labels):\n",
      "        \n",
      "        if cluster_center_label == -1 or cluster_center_label > 100:\n",
      "            # outlier cluster\n",
      "            # so really every point is its own cluster...\n",
      "            D = 0\n",
      "        else:\n",
      "            D = dist_within[cluster_center_label]\n",
      "            \n",
      "        cluster_center = cluster_centers[cluster_center_label_i]\n",
      "        members = (cluster_labels == cluster_center_label)\n",
      "\n",
      "        N0 = np.sum(members)\n",
      "        S = np.sum(skill[members])\n",
      "            \n",
      "        # ['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within']\n",
      "        \n",
      "        cluster_catalog_i = [catalog[category][cati] for category in categories] + \\\n",
      "                            [cluster_center_label, cluster_ID, cluster_center[0], cluster_center[1], N0, S, D]\n",
      "        cluster_catalog.append(cluster_catalog_i)\n",
      "        \n",
      "        cluster_ID += 1\n",
      "    \n",
      " \n",
      "cluster_catalog = pd.DataFrame(cluster_catalog, columns=categories + cluster_catalog_labels)\n",
      "\n",
      "# now add a column for what the filename will be:\n",
      "\n",
      "# save catalog\n",
      "cluster_catalog.to_csv(cluster_catalog_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Merge cluster catalogs into Super Catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from glob import glob\n",
      "\n",
      "max_stage = None  # check on the bugs\n",
      "catalog_list = []\n",
      "for stage in [1,2]:\n",
      "\n",
      "    out_path = '../catalog/stage{0}/'.format(stage)\n",
      "    file_path = out_path + 'cluster_catalog.csv'\n",
      "\n",
      "    df = pd.read_csv(file_path)\n",
      "    # add stage marker\n",
      "    df['stage'] = stage\n",
      "    # also if stage2, up cluster_ID to be max of cluster_ID of stage1 + 1 + oldval\n",
      "    if stage == 1:\n",
      "        max_stage = max(df['cluster_ID'])\n",
      "    if stage == 2:\n",
      "        df['cluster_ID'] += max_stage + 1\n",
      "    catalog_list.append(df)\n",
      "catalog = catalog_list[0].append(catalog_list[1], ignore_index=True)\n",
      "\n",
      "out_path = '../catalog/'\n",
      "catalog.to_csv(out_path + 'cluster_catalog.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Merge catalog without annotations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "catalog_list = []\n",
      "for stage in [1,2]:\n",
      "\n",
      "    out_path = '../catalog/stage{0}/'.format(stage)\n",
      "    file_path = out_path + 'catalog.csv'\n",
      "\n",
      "    df = pd.read_csv(file_path, usecols=categories)\n",
      "    # add stage marker\n",
      "    df['stage'] = stage\n",
      "    # also if stage2, up cluster_ID to be max of cluster_ID of stage1 + 1 + oldval\n",
      "    catalog_list.append(df)\n",
      "catalog = catalog_list[0].append(catalog_list[1], ignore_index=True)\n",
      "\n",
      "out_path = '../catalog/'\n",
      "catalog.to_csv(out_path + 'catalog_unannotated.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Find all training lens cutouts\n",
      "- step through all field pngs\n",
      "- if it has an alpha component, segment and detect objects\n",
      "- make cutouts of all objects"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.ndimage import label, center_of_mass\n",
      "out_path = '../catalog/'\n",
      "images_path = '../images/'\n",
      "cluster_catalog = pd.read_csv(out_path + 'cluster_catalog.csv')\n",
      "catalog = pd.read_csv(out_path + 'catalog_unannotated.csv')\n",
      "for cati in range(len(catalog)):\n",
      "    if catalog['kind'][cati] == 'sim':\n",
      "        outname_field = images_path + 'field_{0}.png'.format(catalog['ID'][cati])\n",
      "        stage = catalog['stage'][cati]\n",
      "\n",
      "        im = get_online_png(catalog['location'][cati], outname_field)\n",
      "        im_mask = im[:,:,3].copy()\n",
      "                \n",
      "        im_mask -= im_mask.mean()\n",
      "        im_mask /= im_mask.std()\n",
      "        im_mask[im_mask < 5] = 0\n",
      "        im_mask[im_mask >= 5] = 1\n",
      "\n",
      "        label_im, nb_labels = label(im_mask)\n",
      "        cms = center_of_mass(im_mask, label_im, range(1,nb_labels+1))\n",
      "        for cluster_i, cm in enumerate(cms):\n",
      "            y, x = cm  # I think this is correct\n",
      "\n",
      "            min_x = np.int(np.floor(x - stamp_size / 2))\n",
      "            max_x = np.int(np.ceil(x + stamp_size / 2))\n",
      "            min_y = np.int(np.floor(y - stamp_size / 2))\n",
      "            max_y = np.int(np.ceil(y + stamp_size / 2))\n",
      "            pad_min_x = 0\n",
      "            pad_max_x = 0\n",
      "            pad_min_y = 0\n",
      "            pad_max_y = 0\n",
      "            pad_trip = False\n",
      "            if min_x < 0:\n",
      "                pad_min_x = -min_x\n",
      "                min_x = 0\n",
      "                pad_trip = True\n",
      "            if max_x >= im.shape[1]:\n",
      "                pad_max_x = max_x - im.shape[1]\n",
      "                max_x = im.shape[1]\n",
      "                pad_trip = True\n",
      "            if min_y < 0:\n",
      "                pad_min_y = -min_y\n",
      "                min_y = 0\n",
      "                pad_trip = True\n",
      "            if max_y >= im.shape[0]:\n",
      "                pad_max_y = max_y - im.shape[0]\n",
      "                max_y = im.shape[0]\n",
      "                pad_trip = True\n",
      "\n",
      "            im_cut = np.pad(im[min_y: max_y, min_x: max_x],\n",
      "                            ((pad_min_y, pad_max_y), (pad_min_x, pad_max_x), (0, 0)),\n",
      "                            mode='constant')\n",
      "            im_cut = im_cut[:stamp_size, :stamp_size, :3]\n",
      "\n",
      "\n",
      "            outname_cluster = images_path + 'cutout__flavor_{0}__ID_{1}__clusteri_{2}__alpha_{3}__stage_{4}.png'.format(catalog['flavor'][cati],\n",
      "                                                                              catalog['ID'][cati],\n",
      "                                                                              cluster_i,\n",
      "                                                                              2,\n",
      "                                                                              stage)\n",
      "\n",
      "            imsave(outname_cluster, im_cut)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Cutouts From Catalog\n",
      "\n",
      "add an 'in_alpha' parameter to cluster catalog"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pylab as plt\n",
      "#['cluster_label', 'cluster_ID', 'x', 'y', 'num_markers', 'skill_sum', 'dist_within']\n",
      "cluster_catalog = pd.read_csv(out_path + 'cluster_catalog.csv')\n",
      "#for clusteri in range(len(cluster_catalog)):\n",
      "for clusteri in range(20093, 196840)[::-1]:\n",
      "    outname_field = images_path + 'fields/field_{0}.png'.format(cluster_catalog['ID'][clusteri])\n",
      "    stage = cluster_catalog['stage'][clusteri]\n",
      "\n",
      "    im = get_online_png(cluster_catalog['location'][clusteri], outname_field)\n",
      "    \n",
      "    x = cluster_catalog['x'][clusteri]\n",
      "    y = cluster_catalog['y'][clusteri]\n",
      "    \n",
      "    if np.int(np.ceil(x)) >= im.shape[1]:\n",
      "        print('Mislabled cluster? {0} {1} {2}'.format(clusteri, x, y))\n",
      "        continue\n",
      "    elif np.int(np.ceil(y)) >= im.shape[0]:\n",
      "        print('Mislabled cluster? {0} {1} {2}'.format(clusteri, x, y))\n",
      "        continue\n",
      "    elif np.int(np.floor(x)) < 0:\n",
      "        print('Mislabled cluster? {0} {1} {2}'.format(clusteri, x, y))\n",
      "        continue        \n",
      "    elif np.int(np.floor(y)) < 0:\n",
      "        print('Mislabled cluster? {0} {1} {2}'.format(clusteri, x, y))\n",
      "        continue       \n",
      "        \n",
      "    min_x = np.int(np.floor(x - stamp_size / 2))\n",
      "    max_x = np.int(np.ceil(x + stamp_size / 2))\n",
      "    min_y = np.int(np.floor(y - stamp_size / 2))\n",
      "    max_y = np.int(np.ceil(y + stamp_size / 2))\n",
      "    pad_min_x = 0\n",
      "    pad_max_x = 0\n",
      "    pad_min_y = 0\n",
      "    pad_max_y = 0\n",
      "    pad_trip = False\n",
      "    if min_x < 0:\n",
      "        pad_min_x = -min_x\n",
      "        min_x = 0\n",
      "        pad_trip = True\n",
      "    if max_x >= im.shape[1]:\n",
      "        pad_max_x = max_x - im.shape[1]\n",
      "        max_x = im.shape[1]\n",
      "        pad_trip = True\n",
      "    if min_y < 0:\n",
      "        pad_min_y = -min_y\n",
      "        min_y = 0\n",
      "        pad_trip = True\n",
      "    if max_y >= im.shape[0]:\n",
      "        pad_max_y = max_y - im.shape[0]\n",
      "        max_y = im.shape[0]\n",
      "        pad_trip = True\n",
      "    \n",
      "    im_cut = np.pad(im[min_y: max_y, min_x: max_x],\n",
      "                    ((pad_min_y, pad_max_y), (pad_min_x, pad_max_x), (0, 0)),\n",
      "                    mode='constant')\n",
      "    im_cut = im_cut[:stamp_size, :stamp_size, :3]\n",
      "    \n",
      "    # now check if the center is in a high alpha region (if alpha is included, as it is with training)\n",
      "    alpha = 0\n",
      "    if im.shape[2] == 4:\n",
      "        im_mask = im[:,:,3].copy()\n",
      "                \n",
      "        im_mask -= im_mask.mean()\n",
      "        im_mask /= im_mask.std()\n",
      "        im_mask[im_mask < 5] = 0\n",
      "        im_mask[im_mask >= 5] = 1\n",
      "        if im_mask[y, x] == 1:\n",
      "            alpha = 1\n",
      "    \n",
      "    outname_cluster = images_path + 'cutouts/cutout__flavor_{0}__ID_{1}__clusteri_{2}__alpha_{3}__stage_{4}__clusterlabel_{5}.png'.format(\n",
      "                                                                      cluster_catalog['flavor'][clusteri],\n",
      "                                                                      cluster_catalog['ID'][clusteri],\n",
      "                                                                      cluster_catalog['cluster_ID'][clusteri],\n",
      "                                                                      alpha,\n",
      "                                                                      stage,\n",
      "                                                                      cluster_catalog['cluster_label'][clusteri])\n",
      "    \n",
      "    imsave(outname_cluster, im_cut)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(clusteri, outname_field, cluster_catalog['stage'][clusteri])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(196840, '../images/field_5183f151e4bb2102190405ae.png', 1)\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since this took so long and I'm at 17 gb with something like 150k more clusters to go, let's stop and assess our distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first let's move all the fields into one directory\n",
      "# and cutouts into another\n",
      "# first we need to find the files\n",
      "from glob import glob\n",
      "base_dir = '/Users/cpd/ipynbs/cs231n/strongcnn/images/'\n",
      "fields = glob(base_dir + 'field*.png')\n",
      "cutouts = glob(base_dir + 'cutout*.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now move\n",
      "import shutil\n",
      "for field in fields:\n",
      "    file_name = field.split(base_dir)[-1]\n",
      "    shutil.move(field, base_dir + 'fields/' + file_name)\n",
      "for field in cutouts:\n",
      "    file_name = field.split(base_dir)[-1]\n",
      "    shutil.move(field, base_dir + 'cutouts/' + file_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# okay now let's create a catalog for the clusters actually used...\n",
      "#20093, 196840\n",
      "# not sure about loc vs iloc\n",
      "range_vals = range(0, 20093) + range(196840, len(cluster_catalog))\n",
      "cluster_catalog_made = cluster_catalog.loc[range_vals][cluster_catalog.columns[2:]]\n",
      "# save it\n",
      "out_path = '../catalog/'\n",
      "cluster_catalog_made.to_csv(out_path + 'cluster_catalog_used.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# take cluster catalog and add column for filename. Note that the alpha parameter is undetermined?!\n",
      "import os\n",
      "filenames = []\n",
      "keeps = []\n",
      "drive_dir = '/Users/cpd/Google Drive/cs231n/cutouts/'\n",
      "for clusteri in xrange(len(cluster_catalog_made)):\n",
      "    entry = cluster_catalog_made.iloc[clusteri]\n",
      "\n",
      "    alphas = []\n",
      "    filenames_alpha = []\n",
      "    for ith in range(3):\n",
      "        if ith < 2:\n",
      "            filename = 'cutout__flavor_{0}__ID_{1}__clusteri_{2}__alpha_{3}__stage_{4}__clusterlabel_{5}.png'.format(\n",
      "                entry['flavor'],\n",
      "                entry['ID'],\n",
      "                entry['cluster_ID'],\n",
      "                ith,\n",
      "                entry['stage'],\n",
      "                entry['cluster_label'])\n",
      "        else:\n",
      "            filename = 'cutout__flavor_{0}__ID_{1}__clusteri_{2}__alpha_{3}__stage_{4}.png'.format(\n",
      "                entry['flavor'],\n",
      "                entry['ID'],\n",
      "                entry['cluster_ID'],\n",
      "                ith,\n",
      "                entry['stage'],\n",
      "                )            \n",
      "        if os.path.exists(drive_dir + filename):\n",
      "            alphas.append(ith)\n",
      "            filenames_alpha.append(filename)\n",
      "    \n",
      "    if len(alphas) == 1:\n",
      "        alpha = alphas[0]\n",
      "\n",
      "        filenames.append(filenames_alpha[0])\n",
      "        keeps.append(clusteri)\n",
      "    elif len(alphas) == 2:\n",
      "        # SHOULD be that you have a 1 and 2 entry. Let's make sure\n",
      "        if 0 in alphas:\n",
      "            # implies that we have, for the same cluster, clicked both on lens and not. What?\n",
      "            print(alphas)\n",
      "            raise Exception('Watch out!')\n",
      "            \n",
      "        alpha = alphas[0]\n",
      "        filenames.append(filenames_alpha[0])\n",
      "        keeps.append(clusteri)\n",
      "    elif len(alphas) > 2:\n",
      "        print(alphas)\n",
      "        raise Exception('Watch out!')\n",
      "    else:\n",
      "        continue\n",
      "        \n",
      "\n",
      "# save only the keeps\n",
      "cluster_catalog_made = cluster_catalog_made.iloc[keeps]\n",
      "cluster_catalog_made['image_location'] = filenames\n",
      "cluster_catalog_made.to_csv(out_path + 'catalog.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:56: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: still not accounting for the alpha == 2 ones..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(np.unique(cc['kind']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['dud' 'sim' 'test']\n"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now let's print some essential stats:\n",
      "cc = cluster_catalog_made\n",
      "\n",
      "print(cc.columns)\n",
      "\n",
      "# get number of objects in stage 1 vs stage 2\n",
      "print(cc.groupby(['stage']).apply(len))\n",
      "\n",
      "print(cc.groupby(['kind']).apply(len))\n",
      "\n",
      "print(cc.groupby(['stage', 'kind']).apply(len))\n",
      "\n",
      "# get number of flavors\n",
      "print(cc.groupby(['flavor']).apply(len))\n",
      "\n",
      "\n",
      "# combine together\n",
      "print(cc.groupby(['stage', 'flavor']).apply(len))\n",
      "print(cc.groupby(['stage', 'flavor', 'status']).apply(len))\n",
      "\n",
      "# also show median probability\n",
      "print(cc.groupby(['stage', 'flavor'])['mean_probability'].apply(np.mean))\n",
      "print(cc.groupby(['stage', 'flavor', 'status'])['mean_probability'].apply(np.mean))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'ID', u'ZooID', u'location', u'mean_probability', u'category', u'kind', u'flavor', u'state', u'status', u'truth', u'cluster_label', u'cluster_ID', u'x', u'y', u'num_markers', u'skill_sum', u'dist_within', u'stage', u'image_location'], dtype='object')\n",
        "stage\n",
        "1        104030\n",
        "2         10824\n",
        "dtype: int64\n",
        "kind\n",
        "dud       3738\n",
        "sim       6708\n",
        "test    104408\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  kind\n",
        "1      dud      2626\n",
        "       sim      5966\n",
        "       test    95438\n",
        "2      dud      1112\n",
        "       sim       742\n",
        "       test     8970\n",
        "dtype: int64\n",
        "flavor\n",
        "dud                  3738\n",
        "lensed galaxy        2722\n",
        "lensed quasar        1935\n",
        "lensing cluster      2051\n",
        "test               104408\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  flavor         \n",
        "1      dud                 2626\n",
        "       lensed galaxy       2030\n",
        "       lensed quasar       1935\n",
        "       lensing cluster     2001\n",
        "       test               95438\n",
        "2      dud                 1112\n",
        "       lensed galaxy        692\n",
        "       lensing cluster       50\n",
        "       test                8970\n",
        "dtype: int64\n",
        "stage  flavor           status   \n",
        "1      dud              rejected      2577\n",
        "                        undecided       49\n",
        "       lensed galaxy    detected      1889\n",
        "                        rejected        75\n",
        "                        undecided       66\n",
        "       lensed quasar    detected      1874\n",
        "                        rejected        21\n",
        "                        undecided       40\n",
        "       lensing cluster  detected      1842\n",
        "                        rejected        99\n",
        "                        undecided       60\n",
        "       test             detected      2067\n",
        "                        rejected     93371\n",
        "2      dud              rejected       697\n",
        "                        undecided      415\n",
        "       lensed galaxy    detected       446\n",
        "                        rejected         3\n",
        "                        undecided      243\n",
        "       lensing cluster  detected        13\n",
        "                        undecided       37\n",
        "       test             detected       150\n",
        "                        rejected      2390\n",
        "                        undecided     6430\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  flavor         \n",
        "1      dud                0.000971\n",
        "       lensed galaxy      0.937048\n",
        "       lensed quasar      0.969475\n",
        "       lensing cluster    0.925185\n",
        "       test               0.021606\n",
        "2      dud                0.019589\n",
        "       lensed galaxy      0.837447\n",
        "       lensing cluster    0.502393\n",
        "       test               0.164020\n",
        "Name: mean_probability, dtype: float64\n",
        "stage  flavor           status   \n",
        "1      dud              rejected     2.153533e-08\n",
        "                        undecided    5.201663e-02\n",
        "       lensed galaxy    detected     9.999156e-01\n",
        "                        rejected     3.022865e-08\n",
        "                        undecided    2.025261e-01\n",
        "       lensed quasar    detected     9.999502e-01\n",
        "                        rejected     2.196958e-08\n",
        "                        undecided    5.068613e-02\n",
        "       lensing cluster  detected     9.999271e-01\n",
        "                        rejected     3.412078e-08\n",
        "                        undecided    1.571418e-01\n",
        "       test             detected     9.975930e-01\n",
        "                        rejected     2.683701e-08\n",
        "2      dud              rejected     1.527905e-09\n",
        "                        undecided    5.249022e-02\n",
        "       lensed galaxy    detected     9.983257e-01\n",
        "                        rejected     5.608823e-18\n",
        "                        undecided    5.525118e-01\n",
        "       lensing cluster  detected     9.832626e-01\n",
        "                        undecided    3.334394e-01\n",
        "       test             detected     9.965460e-01\n",
        "                        rejected     3.193572e-09\n",
        "                        undecided    2.055648e-01\n",
        "Name: mean_probability, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat for the bigger catalog\n",
      "cc_all = pd.read_csv('/Users/cpd/Google Drive/cs231n/' + 'cluster_catalog.csv')\n",
      "cc = cc_all\n",
      "\n",
      "# now let's print some essential stats:\n",
      "\n",
      "\n",
      "print(cc.columns)\n",
      "\n",
      "# get number of objects in stage 1 vs stage 2\n",
      "print(cc.groupby(['stage']).apply(len))\n",
      "\n",
      "print(cc.groupby(['kind']).apply(len))\n",
      "\n",
      "print(cc.groupby(['stage', 'kind']).apply(len))\n",
      "\n",
      "# get number of flavors\n",
      "print(cc.groupby(['flavor']).apply(len))\n",
      "\n",
      "\n",
      "# combine together\n",
      "print(cc.groupby(['stage', 'flavor']).apply(len))\n",
      "print(cc.groupby(['stage', 'flavor', 'status']).apply(len))\n",
      "\n",
      "# also show median probability\n",
      "print(cc.groupby(['stage', 'flavor'])['mean_probability'].apply(np.mean))\n",
      "print(cc.groupby(['stage', 'flavor', 'status'])['mean_probability'].apply(np.mean))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'Unnamed: 0', u'Unnamed: 0.1', u'ID', u'ZooID', u'location', u'mean_probability', u'category', u'kind', u'flavor', u'state', u'status', u'truth', u'cluster_label', u'cluster_ID', u'x', u'y', u'num_markers', u'skill_sum', u'dist_within', u'stage'], dtype='object')\n",
        "stage\n",
        "1        281507\n",
        "2         10966\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kind\n",
        "dud       9586\n",
        "sim      16651\n",
        "test    266236\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  kind\n",
        "1      dud       8438\n",
        "       sim      15884\n",
        "       test    257185\n",
        "2      dud       1148\n",
        "       sim        767\n",
        "       test      9051\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "flavor\n",
        "dud                  9586\n",
        "lensed galaxy        6006\n",
        "lensed quasar        5303\n",
        "lensing cluster      5342\n",
        "test               266236\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  flavor         \n",
        "1      dud                  8438\n",
        "       lensed galaxy        5305\n",
        "       lensed quasar        5303\n",
        "       lensing cluster      5276\n",
        "       test               257185\n",
        "2      dud                  1148\n",
        "       lensed galaxy         701\n",
        "       lensing cluster        66\n",
        "       test                 9051\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  flavor           status   \n",
        "1      dud              detected         31\n",
        "                        rejected       8174\n",
        "                        undecided       233\n",
        "       lensed galaxy    detected       4905\n",
        "                        rejected        202\n",
        "                        undecided       198\n",
        "       lensed quasar    detected       5130\n",
        "                        rejected         59\n",
        "                        undecided       114\n",
        "       lensing cluster  detected       4862\n",
        "                        rejected        240\n",
        "                        undecided       174\n",
        "       test             detected       5830\n",
        "                        rejected     251355\n",
        "2      dud              rejected        718\n",
        "                        undecided       430\n",
        "       lensed galaxy    detected        450\n",
        "                        rejected          3\n",
        "                        undecided       248\n",
        "       lensing cluster  detected         29\n",
        "                        undecided        37\n",
        "       test             detected        151\n",
        "                        rejected       2414\n",
        "                        undecided      6486\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "stage  flavor         \n",
        "1      dud                0.003976\n",
        "       lensed galaxy      0.930905\n",
        "       lensed quasar      0.969942\n",
        "       lensing cluster    0.925002\n",
        "       test               0.022620\n",
        "2      dud                0.019755\n",
        "       lensed galaxy      0.837817\n",
        "       lensing cluster    0.619899\n",
        "       test               0.163833\n",
        "Name: mean_probability, dtype: float64\n",
        "stage  flavor           status   \n",
        "1      dud              detected     9.999216e-01\n",
        "                        rejected     2.181774e-08\n",
        "                        undecided    1.094402e-02\n",
        "       lensed galaxy    detected     9.999511e-01\n",
        "                        rejected     2.858481e-08\n",
        "                        undecided    1.701469e-01\n",
        "       lensed quasar    detected     9.999517e-01\n",
        "                        rejected     2.562131e-08\n",
        "                        undecided    1.214938e-01\n",
        "       lensing cluster  detected     9.999113e-01\n",
        "                        rejected     2.887855e-08\n",
        "                        undecided    1.077213e-01\n",
        "       test             detected     9.978750e-01\n",
        "                        rejected     2.693320e-08\n",
        "2      dud              rejected     1.663273e-09\n",
        "                        undecided    5.274240e-02\n",
        "       lensed galaxy    detected     9.983406e-01\n",
        "                        rejected     5.608823e-18\n",
        "                        undecided    5.566785e-01\n",
        "       lensing cluster  detected     9.853819e-01\n",
        "                        undecided    3.334394e-01\n",
        "       test             detected     9.965689e-01\n",
        "                        rejected     3.185297e-09\n",
        "                        undecided    2.054230e-01\n",
        "Name: mean_probability, dtype: float64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 230
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## So in sum:\n",
      "\n",
      "We have something like 6700 simulated lenses, 3700 known duds, 104408 'tests' (of which we can probably safely use 93000 as duds from the stage 1 rejected tests).\n",
      "\n",
      "Data augmentation for these are clearly needed!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Augment the data: Take the images, apply a reasonable affine transformation\n",
      "\n",
      "One guy actually did this in the convnet for the galaxy zoo morphologies -- we don't have to necessarily store 32x of these data if we don't want (also helps combat overfitting)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}